Gobuster: after performing service scanning, often run into web services running on ports 80 and 443. Web services host web applications which often provide a considerable attack surface and a very high value target during a penetration test. Proper web enumeration is critical, especially when an organization is not exposing many services or those services are appropriately patched. Gobuster is a useful tool to perform the directory enumeration. Sometimes we will find hidden functionality or pages/directories exposing sensitive data that can be leveraged to access the web application or even remote code execution on the web server itself. 

It is a versatile tool that allows for performing DNS, vhost, and directory brute forcing. The tool has additional functionality, such as enumeration of public AWS S3 buckets. For this module’s purposes, we are interested in the directory brute-forcing modes specified with the switch dir. Let us run a simple scan using dirb common. Txt wordlist. 
```
gobuster dir -u http://10.10.10.121/ -w /usr/share/seclists/Discovery/Web-content/common.txt


===============================================================
Gobuster v3.0.1
by OJ Reeves (@TheColonial) & Christian Mehlmauer (@_FireFart_)
===============================================================
[+] Url:            http://10.10.10.121/
[+] Threads:        10
[+] Wordlist:       /usr/share/seclists/Discovery/Web-Content/common.txt
[+] Status codes:   200,204,301,302,307,401,403
[+] User Agent:     gobuster/3.0.1
[+] Timeout:        10s
===============================================================
2020/12/11 21:47:25 Starting gobuster
===============================================================
/.hta (Status: 403)
/.htpasswd (Status: 403)
/.htaccess (Status: 403)
/index.php (Status: 200)
/server-status (Status: 403)
/wordpress (Status: 301)
===============================================================
2020/12/11 21:47:46 Finished
===============================================================
```
Http status code 200 reveals that the resource’s request was successful, while a 403 HTTP Status code indicates that we are forbidden to access the resource. 301 status code indicates that we are being redirected, which is not a failure case. The scan was completed successfully, and it identifies a WordPress installation at /wordpress. Wordpress is the most commonly used CMS (content Management System) and has an enormous potential attack surface. In this case, visiting http://10.10.10.121/wordpress in a browser reveals the Wordpress is still in setup mode, which will allow us to gain remote code execution (RCE) on the server. 

DNS Subdomain Enumeration
There are also may be essential resources hosted on subdomains, such as admin panels or applications with additional functionality that can be exploited. Gobuster can be used to enumerate available subdomains of a given domain using the dns flag to specify DNS mode. First, let us clone the SecLists Github repo, which contains many useful lists for fuzzing and exploitation. Next, add a DNS server such as 1.1.1.1 to the /etc/resolv. Conf file. We will target the domain inlanefreight. Com, the website for a fictional freight and logistics company. 

```
gobuster dns -d inlanefreight.com -w /usr/share/SecLists/Discovery/DNS/namelist.txt
```
This scan reveals several interesting subdomains that we could examine further. 

---
Web Enumeration Tips

#### Banner grabbing / Web Server Headers
Web server headers provide a good picture of what is hosted on a web server. They can reveal the specific application framework in use, the authentication options, and whether the server is missing essential security options or has been misconfigured. We can use cUrl to retrieve the server header information from the command line. CUrl is another essential addition to out presentation testing toolkit, and familiarity with its many options is encouraged. 
```
curl -IL https://www.inlanefreight.com


HTTP/1.1 200 OK
Date: Fri, 18 Dec 2020 22:24:05 GMT
Server: Apache/2.4.29 (Ubuntu)
Link: <https://www.inlanefreight.com/index.php/wp-json/>; rel="https://api.w.org/"
Link: <https://www.inlanefreight.com/>; rel=shortlink
Content-Type: text/html; charset=UTF-8
```
Another handy tool is EyeWitness, which can be used to take screenshots of target web applications, fingerprint them, and identify possible default credentials. 

#### Certificates
SSL/ TLS certificates are another potentially valuable source of information if HTTPS is in use. Browsing to https://10.10.10.121/ and viewing the certificate reveals the details below, including the email address and company name. These could potentially be used to conduct a phishing attack if this is within the scope of an assessment. 

#### Robots. Txt
It is common for websites to contain a robots. Txt file, whose purpose is to instruct search engine web crawlers such as Googlebot which resources can and cannot be accessed for indexing. The robots. Txt file can provide valuable information such as the location of private files and admin pages. In this cases, we see that the robots. Txt file contain two disallowed entries. 
![Robots.txt file disallowing access to /private and /uploaded_files for all user agents.](https://academy.hackthebox.com/storage/modules/77/robots.png)
Now it is possible to navigate to http://10.10.10.121/private 

#### Source Code
It is also worth checking the source code for any web pages we come across.

Question:

Try running some of the web enumeration techniques you learned in this section on the server above, and use the info you get to get the flag.

First, understand how gobuster actually works. It is a bruteforce searching using the common. Txt. So first, we need to find out where the common. Txt is residing in the given OS.
Then, running over the gobuster test with the common. Txt, it is possible to detect allowed access with HTTP 200 status. In this case, it was robots. Txt and the login. Php. Robots. Txt file is for web crawlers, to notify what to crawl or not, and in the robots. Txt, we can find out the admin-login-page. Php file is not allowed in the crawling. In the page code source, it is possible to find out the credential information. 
